# Retrieving Similar Segmentation Problems for Evolutionary Learning

This repository contains the code and scripts for the paper  
**_"Have I Solved This Before? Retrieving Similar Segmentation Problems for Evolutionary Learning"_**

---

## ğŸ§¾ Abstract

This study proposes a novel approach to improve the efficiency of segmentation model training by leveraging previously solved segmentation problems. Instead of focusing solely on algorithm design, this method emphasizes **understanding the problem domain** through dataset analysis.

ğŸ” **Key Ideas**:
- Modern production systems demand self-adaptive, self-configuring software.
- Early-stage design is often done under uncertainty, limiting downstream flexibility.
- The approach focuses on understanding and **comparing datasets**, enabling reuse of prior configurations.
- A **centralized knowledge base** evolves over time, supporting retrieval of similar solutions.
- **Model reuse** reduces training effort, shortens development time, and avoids late-stage redesign.
- Simple, well-fitted models are favored to balance complexity, reliability, and resource constraints.

---

## ğŸ“ Project Structure

```bash
taskRetrieval/
â”œâ”€â”€ main.py                # Main entry point to run the pipeline
â”œâ”€â”€ experiment_params_data.py  # Dataset configurations & experiment settings
â”œâ”€â”€ env_vars.py            # Environment variables & file paths
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ results/               # Output directory for experiment results
â”œâ”€â”€ datasets/              # Input datasets
â”œâ”€â”€ models/                # Model definitions
â””â”€â”€ scripts/               # Scripts for training, evaluation, and utilities
```

## ğŸš€ Approach Overview
The project pipeline analyzes segmentation datasets by extracting feature embeddings, 
computing similarities, and evaluating 
segmentation performance statistically.

#### ğŸ”§ Steps:
* Convert images to grayscale
* Extract feature embeddings using a ResNet model
* Compute the cosine similarity between feature vectors
* Cross-Apply filter pipelines (CGP) to datasets
* Compute the spearman corellation between dataset features and segmentation performance

### ğŸ§© 1. Feature Extraction
Goal: Represent each dataset through a vector of statistical and visual features.

Use a pre-trained CNN (e.g., **ResNet-18**, **ResNet-50**, or **VGG-16**) to embed each image into a fixed-dimensional vector space.

For example, remove the final classification layer and use the **penultimate layer** (e.g., a 512-dimensional vector for ResNet-18).

Let each image  
&nbsp;&nbsp;&nbsp;&nbsp;_x_ âˆˆ â„<sup>HÃ—WÃ—3</sup>  
be mapped to an embedding  
&nbsp;&nbsp;&nbsp;&nbsp;_f(x)_ âˆˆ â„<sup>d</sup>.


### ğŸ” 3. Similarity Computation
Goal: Measure pairwise similarity within each dataset cluster.

* Compute cosine similarity between dataset feature vectors
* Cosine Similarity:

&nbsp;&nbsp;&nbsp;&nbsp;_cosine(x, y) = (x â‹… y) / (||x|| ||y||)_

#### Interpretation
* Empirically in computer vision, cosine similarities above `> 0.7` are typically considered "highly similar" in embedding space.
* `0.5 â€“ 0.6` is moderate, but could be meaningful if your downstream task (CGP pipeline transfer) is sensitive to partial overlap in features.
* `< 0.5` usually indicates the datasets differ enough that you wouldnâ€™t expect strong cross-domain generalization.

### ğŸ“ 4. MCC Score Computation
Goal: Evaluate segmentation performance on selected dataset pairs.

* Cross-Apply CGP pipelines between each pair of datasets
* Compute the Matthews Correlation Coefficient (MCC) for each pair

&nbsp;&nbsp;&nbsp;&nbsp;_MCC = (TP Ã— TN - FP Ã— FN) / sqrt((TP + FP)(TP + FN)(TN + FP)(TN + FN))_

Where:
* TP = True Positives
* TN = True Negatives
* FP = False Positives
* FN = False Negatives

### ğŸ“Š ğŸ§  Statistical Hypothesis
* Null Hypothesis (H0): There is no correlation between dataset similarity and cross-application performance.
* Alternative Hypothesis (H1): Higher similarity does lead to better cross-application performance.

_If the p-value < 0.05, you reject H0 and conclude that the correlation is statistically significant._

#### Spearman Correlation

* Start with Spearman correlation (rank-based, non-parametric, robust to non-linear relationships):

&nbsp;&nbsp;&nbsp;&nbsp;_Ï = 1 - (6 Î£ d<sub>i</sub><sup>2</sup>) / (n(n<sup>2</sup> - 1))_

Where:
* _d<sub>i</sub>_ = difference between ranks of each pair
* _n_ = number of pairs
* Ï = Spearman correlation coefficient (ranges from -1 to 1)
* If Ï > 0, it indicates a positive correlation between similarity and performance.
* If Ï < 0, it indicates a negative correlation.
* If |Ï| is close to 1, it indicates a strong correlation.
* If |Ï| is close to 0, it indicates a weak correlation.
* If p < 0.05, the correlation is statistically significant.
* If p > 0.05, the correlation is not statistically significant.
 

#### Linear Regression

Fit a simple regression model to quantify how much similarity influences performance:

&nbsp;&nbsp;&nbsp;&nbsp;_y = Î²<sub>0</sub> + Î²<sub>1</sub>x + Îµ_

_Where:_
* _y_ = MCC score (segmentation performance)
* _x_ = similarity score (cosine similarity)
* Î²<sub>0</sub> = intercept
* Î²<sub>1</sub> = slope (how much performance changes with similarity)
* Îµ = error term (residuals)

#### Stratified Group Comparison
Divide dataset pairs into e.g. 3 bins:
* High similarity (top 33%)
* Medium similarity
* Low similarity (bottom 33%)

Then run Mann-Whitney U-tests (non-parametric) between performance distributions of the groups:

*If p < 0.05, high similarity groups statistically outperform low similarity ones.*

### ğŸ¯ Scientific Goals

* ğŸ“š Understand Dataset Similarity

Gain insight into structural and visual similarities across datasets.

* ğŸ§ª Evaluate Segmentation Performance: Measure how performance varies with dataset similarity.

* ğŸ“ Validate Statistically: Use robust methods (correlation, Mann-Whitney-U) to confirm hypotheses.

## ğŸ“– How to Cite
If you use this code or ideas from our paper, please cite:

```bibtex
@misc{margraf2025have,
 title   = {Have I Solved This Before? Retrieving Similar Segmentation Problems for Model Training},
 author  = {Margraf, Andreas and Cui, Henning and Haehner, Joerg},
 year    = {2025},
 eprint  = {0000.00000},
 archivePrefix  = {arXiv},
 primaryClass  = {cs.CS},
 url     = {https://arxiv.org/abs/0000.00000}
}
```