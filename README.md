# Retrieving Similar Segmentation Problems for Model Training

This repository contains the code and scripts for the paper  
**_"Have I Solved This Before? Retrieving Similar Segmentation Problems for Model Training"_**

---

## ğŸ§¾ Abstract

This study proposes a novel approach to improve the efficiency of segmentation model training by leveraging previously solved segmentation problems. Instead of focusing solely on algorithm design, this method emphasizes **understanding the problem domain** through dataset analysis.

ğŸ” **Key Ideas**:
- Modern production systems demand self-adaptive, self-configuring software.
- Early-stage design is often done under uncertainty, limiting downstream flexibility.
- The approach focuses on understanding and **comparing datasets**, enabling reuse of prior configurations.
- A **centralized knowledge base** evolves over time, supporting retrieval of similar solutions.
- **Model reuse** reduces training effort, shortens development time, and avoids late-stage redesign.
- Simple, well-fitted models are favored to balance complexity, reliability, and resource constraints.

---

## ğŸ“ Project Structure

```bash
taskRetrieval/
â”œâ”€â”€ main.py                # Main entry point to run the pipeline
â”œâ”€â”€ experiment_params_data.py  # Dataset configurations & experiment settings
â”œâ”€â”€ env_vars.py            # Environment variables & file paths
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ results/               # Output directory for experiment results
â”œâ”€â”€ datasets/              # Input datasets
â”œâ”€â”€ models/                # Model definitions
â””â”€â”€ scripts/               # Scripts for training, evaluation, and utilities
```

## ğŸš€ Approach Overview
The project pipeline analyzes segmentation datasets by extracting features, clustering them, computing similarities, and evaluating segmentation performance statistically.

### ğŸ§© 1. Feature Extraction
Goal: Represent each dataset through a vector of statistical and visual features.

#### ğŸ”§ Steps:
* Convert images to grayscale
* Compute intensity features: mean, std deviation
* Apply Sobel filter for edge density
* Extract GLCM texture features

```python
dataset_features = np.array(dataset_features)
print("Extracted Features Shape:", dataset_features.shape)  # e.g. (30, 5)
cluster_labels, clustered_datasets = cluster_datasets(dataset_features, dataset_names)
```

ğŸ“¦ Output: Feature vectors representing each dataset.

### ğŸ§­ 2. Clustering Datasets

Goal: Group datasets based on their feature similarity.

#### ğŸ“Š Steps:

* Use Elbow Method to select the optimal number of clusters
* Apply K-Means clustering
* Visualize clusters in 2D using PCA

ğŸ—‚ Output: Cluster labels for all datasets

### ğŸ” 3. Similarity Computation
Goal: Measure pairwise similarity within each dataset cluster.


#### ğŸ“ Steps:
* Compute cosine similarity between dataset feature vectors
* Generate similarity matrices per cluster

```python
print("Computing similarity within clusters...")
similarities, similarity_labels = compute_similarity_within_clusters(dataset_features, cluster_labels, clustered_datasets)
plot_similarity_heatmap(similarities, similarity_labels)
```
ğŸ“¦ Output: Cluster-wise similarity matrices

### ğŸ¤ 4. Dataset Pair Selection

Goal: Select meaningful dataset pairs for evaluation.

#### ğŸ“‹ Steps:

Identify dataset pairs that are:
* Most similar
* Moderately similar
* Least similar
* Store these for performance benchmarking

ğŸ“¦ Output: Dataset pairs for further analysis

### ğŸ“ 5. MCC Score Computation
Goal: Evaluate segmentation performance on selected dataset pairs.

#### âš™ï¸ Steps:

* Generate binary masks to simulate ground truth and predictions
* Compute Matthews Correlation Coefficient (MCC)

```python
print("Computing MCC scores for dataset pairs...")
selected_pairs = select_pairs(similarities, cluster_labels)
print("Selected dataset pairs:", selected_pairs)

raise NotImplementedError
mcc_results = compute_mcc_selected_pairs(selected_pairs)
```

ğŸ“¦ Output: MCC scores for each pair

### ğŸ“Š 6. Statistical Analysis
Goal: Statistically validate how similarity impacts segmentation performance.

#### ğŸ“ˆ Steps:

* Use bootstrapping to compute confidence intervals
* Conduct ANOVA across similarity groups
* Calculate Pearson correlation between similarity and MCC

```python
mcc_values = [m[2] for m in mcc_results]
mean_mcc, ci_low, ci_high = bootstrap_mcc(mcc_values)

print(f"Bootstrap MCC mean: {mean_mcc:.3f} (95% CI: {ci_low:.3f} - {ci_high:.3f})")
sns.histplot(mcc_values, kde=True)
plt.title("Bootstrapped MCC Distribution")
plt.show(block=True)

compute_anova_and_correlation(mcc_results)
```

ğŸ“¦ Output: Statistical insight into the relationship between similarity and performance

### ğŸ¯ Scientific Goals

* ğŸ“š Understand Dataset Similarity

Gain insight into structural and visual similarities across datasets.

* ğŸ§ª Evaluate Segmentation Performance

Measure how performance varies with dataset similarity.

* ğŸ“ Validate Statistically

Use robust methods (bootstrapping, ANOVA, correlation) to confirm hypotheses.

## ğŸ“– How to Cite
If you use this code or ideas from our paper, please cite:

```bibtex
@misc{margraf2025have,
 title   = {Have I Solved This Before? Retrieving Similar Segmentation Problems for Model Training},
 author  = {Margraf, Andreas and Cui, Henning and Haehner, Joerg},
 year    = {2025},
 eprint  = {0000.00000},
 archivePrefix  = {arXiv},
 primaryClass  = {cs.CS},
 url     = {https://arxiv.org/abs/0000.00000}
}
```